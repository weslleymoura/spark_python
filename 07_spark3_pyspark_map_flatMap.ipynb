{"cells":[{"cell_type":"markdown","source":["## Interoperabilidade entre Dataframe e RDD\nNeste notebook veremos como utilizar os comandos .map e .flatMap para transitar entre RDD e Dataframe. Afinal, atrás de todo Dataframe existe um grande RDD :)<br>\nAdicionalmente, aprenderemos a utilizar lambda functions!"],"metadata":{}},{"cell_type":"markdown","source":["### Importando as bibliotecas\nNesta etapa iremos apenas importar todas as bibliotecas e funções necessárias para rodar o programa"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession, Row\nfrom pyspark.sql.functions import col, lit, concat, upper"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["### Criando uma SparkSession\nPor meio de uma SparkSession terei acesso ao SparkContext da minha aplicação."],"metadata":{}},{"cell_type":"code","source":["# Inicia uma sparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Meu curso de pyspark\") \\\n    .getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Carregando dados a partir de um arquivo CSV\nNesta etapa estamos carregando os dados que utilizaremos neste notebook"],"metadata":{}},{"cell_type":"code","source":["dados = spark \\\n    .read.option(\"header\",\"true\") \\\n    .option(\"inferSchema\",\"true\") \\\n    .option(\"delimiter\",\";\") \\\n    .format(\"csv\") \\\n    .load(\"/FileStore/tables/bank_additional_full-3fd09.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### Função map\nSpark Dataframes não possui a função MAP diretamente. Na realidade, esta é uma função do RDD associado ao seu Dataframe.<br>\nÉ por este motivo que, para acionar a função .map, devemos antes referenciar o RDD associado ao Dataframe usando o comando .rdd"],"metadata":{}},{"cell_type":"markdown","source":["**Exemplo de uso da funcao map**<br>\nDica: quando estiver pensando na lógica de escrever este tipo de comando, troque .toDF(['Job']).show() por .take(10) para analisar os resultados!"],"metadata":{}},{"cell_type":"code","source":["dados.rdd.map(lambda row: Row('job: ' + row.job)).toDF(['Job']).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+\n             Job|\n+----------------+\n  job: housemaid|\n   job: services|\n   job: services|\n     job: admin.|\n   job: services|\n   job: services|\n     job: admin.|\njob: blue-collar|\n job: technician|\n   job: services|\njob: blue-collar|\n   job: services|\njob: blue-collar|\n  job: housemaid|\njob: blue-collar|\n    job: retired|\njob: blue-collar|\njob: blue-collar|\njob: blue-collar|\n job: management|\n+----------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["**Outra forma de conseguir o mesmo resultado usando as abordagens que aprendemos até agora**"],"metadata":{}},{"cell_type":"code","source":["dados.select(\"job\").withColumn(\"teste\", concat(lit(\"Job: \"), col(\"job\"))).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------------+\n      job|         teste|\n+---------+--------------+\nhousemaid|Job: housemaid|\n services| Job: services|\n services| Job: services|\n   admin.|   Job: admin.|\n services| Job: services|\n+---------+--------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["**Usando a funcao map para aplicar a funcao upper case**"],"metadata":{}},{"cell_type":"code","source":["# Veja que apenas aplicando a função MAP, teremos de volta uma lista de valores\nres = dados.rdd.map (lambda linha : linha.poutcome.upper()).take(10)\nprint(res)\nprint(type(res))\n\n# Não queremos retornar uma lista de string, mas sim um novo Dataframe com a coluna convertida. É por isso que temos que transformar esta \"lista de strings\" em uma \"lista de Rows\"\ndados.rdd.map (lambda linha : Row(linha.poutcome.upper())).take(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;NONEXISTENT&#39;, &#39;NONEXISTENT&#39;, &#39;NONEXISTENT&#39;, &#39;NONEXISTENT&#39;, &#39;NONEXISTENT&#39;, &#39;NONEXISTENT&#39;, &#39;NONEXISTENT&#39;, &#39;NONEXISTENT&#39;, &#39;NONEXISTENT&#39;, &#39;NONEXISTENT&#39;]\n&lt;class &#39;list&#39;&gt;\nOut[75]: [&lt;Row(&#39;NONEXISTENT&#39;)&gt;,\n &lt;Row(&#39;NONEXISTENT&#39;)&gt;,\n &lt;Row(&#39;NONEXISTENT&#39;)&gt;,\n &lt;Row(&#39;NONEXISTENT&#39;)&gt;,\n &lt;Row(&#39;NONEXISTENT&#39;)&gt;,\n &lt;Row(&#39;NONEXISTENT&#39;)&gt;,\n &lt;Row(&#39;NONEXISTENT&#39;)&gt;,\n &lt;Row(&#39;NONEXISTENT&#39;)&gt;,\n &lt;Row(&#39;NONEXISTENT&#39;)&gt;,\n &lt;Row(&#39;NONEXISTENT&#39;)&gt;]</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# Fazendo o comando completo...\ndados.rdd.map (lambda linha : Row(linha.poutcome.upper())).toDF(['POUT']).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\n       POUT|\n+-----------+\nNONEXISTENT|\nNONEXISTENT|\nNONEXISTENT|\nNONEXISTENT|\nNONEXISTENT|\n+-----------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["**Outra forma de conseguir o mesmo resultado usando as abordagens que aprendemos até agora**"],"metadata":{}},{"cell_type":"code","source":["dados.select(upper(\"poutcome\")).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+\nupper(poutcome)|\n+---------------+\n    NONEXISTENT|\n    NONEXISTENT|\n    NONEXISTENT|\n    NONEXISTENT|\n    NONEXISTENT|\n+---------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["### Função flatMap\nflatMap é uma função muito parecida com .map. A diferença é que os resultados são retornados em um único elemento (flat)<br>\nNos exemplos a seguir retornaremos uma lista com a idade das 10 primeiras pessoas do Dataframe"],"metadata":{}},{"cell_type":"code","source":["# Usando .map\ndados.rdd.map (lambda row : [row.age]).take(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[50]: [[56], [57], [37], [40], [56], [45], [59], [41], [24], [25]]</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# usando .flatMap\ndados.rdd.flatMap (lambda row : [row.age]).take(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[51]: [56, 57, 37, 40, 56, 45, 59, 41, 24, 25]</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["### Obrigado!\nQuer construir uma carreira em Data Science? Acesse meu blog pessoal em https://www.hackinganalytics.com/"],"metadata":{}}],"metadata":{"name":"07_spark3_pyspark_map_flatMap","notebookId":1494187155874360},"nbformat":4,"nbformat_minor":0}
